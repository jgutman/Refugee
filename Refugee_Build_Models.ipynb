{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import scale\n",
    "from StringIO import StringIO\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from inspect import getmembers\n",
    "from IPython.core.display import Image\n",
    "from numpy.random import randn\n",
    "import itertools, operator, sys\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_with_time_series_split_by_judge.csv')\n",
    "test_data = pd.read_csv('test_data_with_time_series_split_by_judge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bool_cols = train_data.dtypes[train_data.dtypes == \"bool\"]\n",
    "train_data[bool_cols.index] = train_data[bool_cols.index].astype(np.float)\n",
    "test_data[bool_cols.index] = test_data[bool_cols.index].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.replace(-1*np.inf, -1000., inplace=True)\n",
    "test_data.replace(-1*np.inf, -1000., inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the 'grantraw' label from train and test records\n",
    "#collist = [x for x in train_data.columns if x not in ('grantraw')]\n",
    "\n",
    "X_train = train_data.drop(\"grantraw\", axis=1)\n",
    "y_train = train_data[\"grantraw\"]\n",
    "\n",
    "X_test  = test_data.drop(\"grantraw\", axis=1)\n",
    "y_test  = test_data[\"grantraw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "identical = []\n",
    "for col in X_train.columns:\n",
    "    if len(X_train[col].unique()) == 1:\n",
    "        identical = identical + [col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = X_train.columns[X_train.max(axis=0) - X_train.min(axis=0) == 1]\n",
    "nonbinary = [col for col in X_train.columns if col not in (list(binary)+identical)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scale all the values between [0,1]\n",
    "X_train[nonbinary] = sklearn.preprocessing.normalize(X_train[nonbinary], axis=0) \n",
    "X_test[nonbinary] = sklearn.preprocessing.normalize(X_test[nonbinary], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_complete = X_train.copy()\n",
    "#y_train_complete = y_train.copy()\n",
    "#X_train = X_train[0:100000]\n",
    "#y_train = y_train[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train percent error:  0.0\n",
      " Test percent error:  45.4483204259\n",
      " Train percent error:  0.0\n",
      " Test percent error:  45.2911165334\n",
      " Train percent error:  0.0\n",
      " Test percent error:  44.3951507904\n",
      " Train percent error:  0.0\n",
      " Test percent error:  45.7675504162\n",
      "1 loops, best of 3: 59.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#SIMPLE DECISION TREE MODEL \n",
    "clf    = DecisionTreeClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Calculate percent Train error\n",
    "y_tr       = clf.predict(X_train)\n",
    "train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)\n",
    "print \" Train percent error: \", train_err\n",
    "        \n",
    "#Calculate percent Test error\n",
    "y_te      = clf.predict(X_test)\n",
    "test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)\n",
    "print \" Test percent error: \", test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum Train Error:  12.6571286801\n",
      "Model parameters->  Criterion: gini, splitter: random, min_num_leaf: 1, min_samples_split: 2, tree_depth: 19\n",
      "\n",
      "Minimum Test Error:  28.2472548466\n",
      "Model parameters->  Criterion: entropy, splitter: random, min_num_leaf: 2, min_samples_split: 3, tree_depth: 6\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE MODELS FOR DIFFERENT PARAMETER VALUES\n",
    "min_percnt_test_err  = 101\n",
    "min_percnt_train_err = 101\n",
    "\n",
    "min_percnt_test_err_parameters = \" \"\n",
    "min_percnt_train_err_parameters = \" \"\n",
    "\n",
    "#count = 1\n",
    "for criterion_type in ['entropy', 'gini']:\n",
    "    for split_type in ['best', 'random']:        \n",
    "        for min_leaf in range(1,4):    \n",
    "            for min_split in range(2,4):\n",
    "                for depth in range(1,20): \n",
    "                    \n",
    "                    #print \"\\nModel-%d\" %count \n",
    "                    #count = count + 1                    \n",
    "                    \n",
    "                    string = \"Criterion: \"+criterion_type +\", splitter: \"+split_type+\", min_num_leaf: \"+str(min_leaf) \n",
    "                    string = string+\", min_samples_split: \"+str(min_split)+\", tree_depth: \"+str(depth)\n",
    "                    #print string\n",
    "                   \n",
    "                    #Build decision tree\n",
    "                    clf = DecisionTreeClassifier(max_depth=depth, criterion=criterion_type, splitter=split_type, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "                    clf.fit(X_train,y_train)\n",
    "\n",
    "                    #Calculate percent train error\n",
    "                    y_tr       = clf.predict(X_train)\n",
    "                    train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)\n",
    "                    #print \"Train Error: \", train_err\n",
    "        \n",
    "                    #Check and save the min train error                    \n",
    "                    if train_err < min_percnt_train_err:\n",
    "                        min_percnt_train_err            = train_err \n",
    "                        min_percnt_train_err_parameters = string\n",
    "               \n",
    "                    #Calculate percent test error\n",
    "                    y_te      = clf.predict(X_test)\n",
    "                    test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)\n",
    "                    #print \"Test Error: \", test_err\n",
    "                    \n",
    "                    #Check and save the min test error\n",
    "                    if test_err < min_percnt_test_err:\n",
    "                        min_percnt_test_err            = test_err \n",
    "                        min_percnt_test_err_parameters = string\n",
    "\n",
    "#Print the minimum percent test and train errors along with their model parameter values\n",
    "print \"\\nMinimum Train Error: \", min_percnt_train_err\n",
    "print \"Model parameters-> \", min_percnt_train_err_parameters\n",
    "\n",
    "print \"\\nMinimum Test Error: \", min_percnt_test_err\n",
    "print \"Model parameters-> \", min_percnt_test_err_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Error:  0.282862650554\n",
      " Test Error:  38.0645596844\n",
      " Train Error:  0.290412365426\n",
      " Test Error:  36.9236259126\n",
      " Train Error:  0.291167336914\n",
      " Test Error:  34.9089085421\n",
      " Train Error:  0.288399108127\n",
      " Test Error:  36.5928226296\n",
      "1 loops, best of 3: 25.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#SIMPLE RANDOM FOREST MODEL \n",
    "clf    = RandomForestClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#Calculate percent Train error\n",
    "y_tr       = clf.predict(X_train)\n",
    "train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)\n",
    "print \" Train Error: \", train_err\n",
    "        \n",
    "#Calculate percent Test error\n",
    "y_te      = clf.predict(X_test)\n",
    "test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)\n",
    "print \" Test Error: \", test_err\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RANDOM FOREST MODEL FOR DIFFERENT PARAMETER VALUES\n",
    "\n",
    "num_features = len(train_data.columns)\n",
    "\n",
    "min_percnt_test_err  = 101\n",
    "min_percnt_train_err = 101\n",
    "\n",
    "min_percnt_test_err_parameters = \" \"\n",
    "min_percnt_train_err_parameters = \" \"\n",
    "\n",
    "#count = 1\n",
    "for criterion_type in ['entropy', 'gini']:\n",
    "    for max_feature in ['sqrt', 'log2', 0.3]: \n",
    "        for min_leaf in range(1,4):    \n",
    "            for min_split in range(2,4):\n",
    "                for depth in range(8,17): \n",
    "                    for n_est in range (7, 14 ):\n",
    "                        for bootstr in [True, False]:\n",
    "                            for warm_start_b in [True, False]:\n",
    "                  \n",
    "                                #print \"\\nModel-%d\" %count \n",
    "                                #count = count + 1                    \n",
    "                    \n",
    "                                string = \"Criterion: \" +criterion_type+ \", max_feature: \"+ str(max_feature) \n",
    "                                string = string+\", min_num_leaf: \" +str(min_leaf)+ \", min_samples_split: \"+str(min_split)\n",
    "                                string = string+\", tree_depth: \"+str(depth)+ \", n_estimators: \"+str(n_est)\n",
    "                                string = string+\", bootstr: \"+str(bootstr)+ \", warm_start_bool: \"+str(warm_start_b)\n",
    "                                #print string\n",
    "                   \n",
    "                                #Build Random Forest\n",
    "                                clf = RandomForestClassifier(criterion=criterion_type, max_features=max_feature, min_samples_leaf=min_leaf, min_samples_split=min_split,max_depth=depth,n_estimators=n_est,bootstrap=bootstr,warm_start=warm_start_b)\n",
    "                                clf.fit(X_train,y_train)\n",
    "\n",
    "                                #Calculate percent train error\n",
    "                                y_tr       = clf.predict(X_train)\n",
    "                                train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)\n",
    "                                #print \"Train Error: \", train_err\n",
    "        \n",
    "                                #Check and save the min train error                    \n",
    "                                if train_err < min_percnt_train_err:\n",
    "                                    min_percnt_train_err            = train_err \n",
    "                                    min_percnt_train_err_parameters = string\n",
    "               \n",
    "                                #Calculate percent test error\n",
    "                                y_te      = clf.predict(X_test)\n",
    "                                test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)\n",
    "                                #print \"Test Error: \", test_err\n",
    "                    \n",
    "                                #Check and save the min test error\n",
    "                                if test_err < min_percnt_test_err:\n",
    "                                    min_percnt_test_err            = test_err \n",
    "                                    min_percnt_test_err_parameters = string\n",
    "\n",
    "#Print the minimum percent test and train errors along with their model parameter values\n",
    "print \"\\nMinimum Train Error: \", min_percnt_train_err\n",
    "print \"Model parameters-> \", min_percnt_train_err_parameters\n",
    "\n",
    "print \"\\nMinimum Test Error: \", min_percnt_test_err\n",
    "print \"Model parameters-> \", min_percnt_test_err_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DEFINE perform_adaboost FUNCTION\n",
    "def perform_adaboost(base_estimator, X_train, y_train, X_test, y_test, num_steps, depth, algorithm_t):\n",
    "        \n",
    "    weight = 1/len(X_train)\n",
    "    weight = [weight]*len(X_train)\n",
    "\n",
    "    train_err_list = list()\n",
    "    test_err_list  = list()    \n",
    "    \n",
    "    if (base_estimator == \"Decision_Tree\"):\n",
    "        classifier = DecisionTreeClassifier(max_depth=depth)\n",
    "    elif (base_estimator == \"Random_Forest\"):\n",
    "        classifier = RandomForestClassifier(max_depth=depth)\n",
    "        \n",
    "    for step in range(1,num_steps+1):       \n",
    "        \n",
    "        bdt = AdaBoostClassifier(classifier,algorithm=algorithm_t,n_estimators=step)     \n",
    "        bdt.fit(X_train, y_train)         \n",
    "        \n",
    "        #Calculate train and test misclassification percent\n",
    "        y_tr       = bdt.predict(X_train)\n",
    "        train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)    \n",
    "        train_err_list.append(train_err)\n",
    "              \n",
    "        y_te      = bdt.predict(X_test)\n",
    "        test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)        \n",
    "        test_err_list.append(test_err) \n",
    "          \n",
    "    return train_err_list, test_err_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum train error:  24.2350314257\n",
      "minimum test error:  29.8710899214\n"
     ]
    }
   ],
   "source": [
    "#BUILD ADABOOST BASED MODEL WITH DECISION TREE AS A WEAK CLASSIFIER\n",
    "base_estimator       = \"Decision_Tree\"\n",
    "num_steps            = 10\n",
    "depth                = 5\n",
    "algorithm_t          = \"SAMME\"\n",
    "\n",
    "train_err_list, test_err_list = perform_adaboost(base_estimator, X_train, y_train, X_test, y_test, num_steps, depth, algorithm_t)\n",
    "print \"minimum train error: \", min(train_err_list)\n",
    "print \"minimum test error: \", min(test_err_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum train error:  24.1344139376\n",
      "minimum test error:  28.7489409771\n"
     ]
    }
   ],
   "source": [
    "#BUILD ADABOOST BASED MODEL WITH RANDOM FOREST AS A WEAK CLASSIFIER\n",
    "base_estimator       = \"Random_Forest\"\n",
    "num_steps            = 10\n",
    "depth                = 5\n",
    "algorithm_t          = \"SAMME\"\n",
    "\n",
    "train_err_list, test_err_list = perform_adaboost(base_estimator, X_train, y_train, X_test, y_test, num_steps, depth, algorithm_t)\n",
    "print \"minimum train error: \", min(train_err_list)\n",
    "print \"minimum test error: \", min(test_err_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DEFINE FUNCTION TO PERFORM STOCHASTIC GRADIENT DESCENT\n",
    "def perform_SGD(X_train, y_train, X_test, y_test, loss_type, penalty_type, shuffle_bool):\n",
    "    \n",
    "    clf = SGDClassifier(loss=loss_type, penalty=penalty_type, shuffle=shuffle_bool)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #Calculate train percent error\n",
    "    y_tr       = clf.predict(X_train)\n",
    "    train_err  = 100 - (accuracy_score(y_train, y_tr, normalize = True) * 100)    \n",
    "    #print \"train_err: \", train_err\n",
    "    \n",
    "    #Calculate test percent error\n",
    "    y_te      = clf.predict(X_test)\n",
    "    test_err  = 100 - (accuracy_score(y_test, y_te, normalize = True) * 100)        \n",
    "    #print \"test_err: \", test_err\n",
    "    \n",
    "    return train_err, test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum Train Error:  25.7897783659\n",
      "Model parameters->  Loss: hinge, Penalty: l1, shuffle:True\n",
      "\n",
      "Minimum Test Error:  41.9888034287\n",
      "Model parameters->  Loss: squared_hinge, Penalty: l1, shuffle:False\n"
     ]
    }
   ],
   "source": [
    "#BUILD MODEL WITH STOCHASTIC GRADIENT DESCENT FOR DIFFERENT LOSS FUNCTIONS AND PENALTY TYPES\n",
    "#LOST TYPE OF 'HINGE' WILL GIVE SOFT-MARGIN LINEAR SVM MODEL\n",
    "\n",
    "min_percnt_test_err  = 101\n",
    "min_percnt_train_err = 101\n",
    "\n",
    "min_percnt_test_err_parameters = \" \"\n",
    "min_percnt_train_err_parameters = \" \"\n",
    "\n",
    "for loss in [\"hinge\", \"modified_huber\", \"squared_hinge\", \"log\"]:\n",
    "    for penalty in [\"l1\", \"l2\", \"elasticnet\"]:\n",
    "        for shuffle in [True, False]:\n",
    "            string = \"Loss: \"+loss+\", Penalty: \"+penalty+\", shuffle:\"+str(shuffle)\n",
    "            train_error, test_error = perform_SGD(X_train, y_train, X_test, y_test, loss, penalty, shuffle)\n",
    "           \n",
    "            #Check and save the min errors \n",
    "            if train_error < min_percnt_train_err:\n",
    "                min_percnt_train_err            = train_error\n",
    "                min_percnt_train_err_parameters = string \n",
    "            \n",
    "            if test_error < min_percnt_test_err:\n",
    "                min_percnt_test_err            = test_error\n",
    "                min_percnt_test_err_parameters = string \n",
    "\n",
    "#Print the minimum percent test and train errors along with their model parameter values\n",
    "print \"\\nMinimum Train Error: \", min_percnt_train_err\n",
    "print \"Model parameters-> \", min_percnt_train_err_parameters\n",
    "\n",
    "print \"\\nMinimum Test Error: \", min_percnt_test_err\n",
    "print \"Model parameters-> \", min_percnt_test_err_parameters            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
